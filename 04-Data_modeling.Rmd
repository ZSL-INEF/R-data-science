# 数据建模

## 关系统计

### 相关分析

#### 概念

直线相关：是一种研究两个变量之前的线性相关关系（相关的方向和密切程度）的一种统计方法

注：相关系数等于0时，只能说明两变量间无直线关系，不能说两变量无关

#### 相关系数及其计算

1.  相关系数：又称Pearson积差相关系数，是说明具有直线相关关系的两个数值变量之间相关的方向和密切程度的统计量。

2.计算公式： $$
r = \frac{l_{XY}}{\sqrt{l_{XX}l_{YY}}} = \frac{\sum (X-\bar{X})(Y-\bar{Y})}{\sqrt{\sum (X-\bar{X})^2\sum (Y-\bar{Y})^2}}
$$ 其中$l_{XY}$表示$X$与$Y$的离均差积和，$l_{XX}$表示$X$的离均差平方和，$l_{YY}$表示$Y$的离均差平方和。

相关系数是没有单位的，取值范围为$[-1,1]$. 当$r > 0$时，两随机变量为正相关；当$r <0$时，两随机变量为负相关; 当$|r| = 1$时，两随机变量完全相关；当$|r| = 0$时, 两随机变量无直线关系.

### 一元与多元回归

#### 概念

1.回归：是分析研究变量与变量之间的关系的一种行为，也可以说是回归于事物本来的面目！

2.回归分析：指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。

回归分析按照涉及的变量的多少，分为一元回归和多元回归分析；按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。

### 普通最小二乘法回归

普通最小二乘法回归（ordinary least squares，OLS）是一类服务于正态响应变量的模型框架。包括：简单线性回归、多项式回归和多元线性回归；OLS回归是目前最常见的统计分析方法。

####简单线性回归模型

简单回归模型 $$
y = \beta_0 +\beta_1x+u
$$ **零条件均值**假设下： $$
E(u) = 0 \\
Cov(x,u) = E(xu) = 0
$$ 矩估计: $$
\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{\beta}_0 -\hat{\beta}_1x_i) = 0 \\
\frac{1}{n}\sum_{i=1}^{n}x_i(y_i-\hat{\beta}_0 -\hat{\beta}_1x_i) = 0 
$$ 得到估计的斜率： $$
\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}{n}(x_i-\bar{x})^2}
$$ 定义总平方和(total sum of squares, SST)，解释平方和(explained sum of squares.SSE), 残差平方和(residual sum of squares, SSR): $$
SST = \sum_{i=1}{n}(y_i-\bar{y})^2\\
SSE = \sum_{i=1}{n}(\hat{y}_i-\bar{y})^2\\
SSR = \sum_{i=1}{n}\hat{u}_i^2\\
SST = SSE + SSR
$$

拟合优度： $$
R^2 = SSE/SST = 1 - SSR/SST
$$

**残差的标准误**是残差平方和的均方根，用$s_e$表示，计算公式为：
$$
s_e = \sqrt{\frac{\sum(y_i - \hat{y_i})^2}{n - k - 1}} = \sqrt{\frac{SSE}{n - k - 1}} = \sqrt{MSE}
$$
式中，k为自变量的个数，在一元线性回归中，n-k-1=n-2.
$s_e$是度量各观测值在直线周围分散程度的一个统计量，它反映了实际观测值$y_i$ 与回归估计值$\hat{y}_i$之间的差异程度. $s_e$也是对误差项$\epsilon$的标准差$\sigma$的估计，它可以看作在排除了x对y的线性影响后，y随机波动大小的一个估计量. 从实际意义看，$s_e$反映了用估计的回归方程预测y时产生的预测误差大小各观测值越靠近直线，$s_e$就越小，根据回归方程进行预测也就越准确:若各观测值全部落在直线上，$s_e = 0$,此时用x预测y是没有误差的.


#### 多元回归分析

设因变量为$y$,$k$个自变量分别为$x_1,x_2,\ldots,x_k,$描述因变量$y$依赖于自变量$x_1,x_2,\ldots,x_k$和误差项$\epsilon$的方程称为**多元线性回归模型**, 一般形式为: $$
y = \beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k + \epsilon
$$ 其中$\beta_0,\beta_1,\ldots,\beta_k$为模型参数。而$\epsilon$是用\$\beta\_0+\beta\_1x_1+\beta\_2x_2+\cdots+\beta\_kx_k $去代替$y\$所形成的误差。

一般在多元线性回归模型中，对误差项$\epsilon$有三个基本假设：

-   **正态性**. $\epsilon$是一个服从均值为0的正态分布的随机变量。

-   **方差齐性**. 对于任意自变量$x_1,x_2,\ldots,x_k$的取值，$\epsilon$的方差$\sigma^2$都一个相同常值。

-   **独立性**. 对自变量不同取值得到的$\epsilon$是独立的。

根据上面三个基本假设，称 $$
E(y) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_kx_k
$$ 为**多元线性回归方程**, 该方程描述了因变量$y$的期望与自变量$x_1,x_2,\ldots,x_k$之间的关系。

多元回归模型中的回归参数$\beta_0,\beta_1,\ldots,\beta_k$是未知的，需要利用现有样本数据去估计，当用样本估计值$\hat{\beta}_0, \hat{\beta}_1,\ldots, \hat{\beta}_k$去估计回归模型中的参数$\beta_0, \beta_1, \beta_2, \ldots, \beta_k$时，就得到了**估计的多元线性回归方程**，其一般形式为 $$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x_1 + \ldots + \hat{\beta_k}x_k
$$ 其中$\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_k$ 为回归参数$\beta_0, \beta_1, \ldots, \beta_k$的估计值；而$\hat{y}$是因变量$y$的估计值。我们记$\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_k$为**偏回归系数**. $\hat{\beta_i}$表示当$x_1,x_2,\ldots,x_{i-1},x_{i+1},\ldots,x_k$不变时，$x_i$每改变一个单位因变量$y$的平均改变量。

#### 参数的最小二乘估计

对于多元线性回归模型中的参数$\beta_0,\beta_1,\ldots,\beta_k$,我们仍然用最小二乘法来估计，即

$$
P = \sum (y_i - \hat{y}_i)^2 = \sum(y_i - \hat{\beta_0} - \hat{\beta}_1x_1 - \cdots - \hat{\beta}_kx_k)^2 
$$ 我们对其求最小值，即 $$
\begin{cases}
\frac{\partial P}{\partial \beta_0}|_{\beta_0 = \hat{\beta}_0} = 0 \\
\frac{\partial P}{\partial \beta_1}|_{\beta_1 = \hat{\beta}_i} = 0, i =1,2,\ldots,k
\end{cases}
$$ \## 拟合优度和显著性检验 \### 模型的拟合优度

多元线性回归模型的拟合优度一般都是用多重决定系数来评估。

##### 多重系数

在多元线性回归中，因变量的总误差平方和SST\$ = \sum(y_i-\overline{y})\^2$可以通过计算从而分成两个具有实际意义的求和项：回归平方和SSR$ = \sum(\hat{y}\_i - \overline{y})\^2$与残差平方和SSE$ = \sum(y_i - \hat{y}\_i)\^2\$. 而我们记：

$$
R^2 = \frac{SSR}{SST}
$$ 为**多重决定系数**，它是多元线性回归中回归平方和占总平方和的比例。$R^2$s是多元线性回归模型拟合优度的度量。

但是如果一个模型中增加一个自变量，即使这个自变量在统计上并不显著，$R^2$也会变大。因此，为了避免增加自变量而使得估计的$R^2$偏高，一般有**调整的多重决定系数**： $$
R^2_a = 1 - (1 - R^2) \times\frac{n-1}{n-k-1}
$$ 由于调整的多重决定系数不受模型自变量个数的影响，故在多元回归分析中一般使用其来评估回归模型的拟合优度。

##### 估计标准误

在多元回归分析中的估计标准误是其残差的标准差，它是多元回归模型中误差项$\epsilon$标准差$\sigma$的一个估计量，它的基本形式为： $$
s_e = \sqrt{\frac{\sum(y_i - \hat{y}_i)^2}{n - k - 1}} = \sqrt{\frac{SSE}{n - k - 1}}
$$ 其中k为自变量的个数。

#### 模型的显著性检验

### 多重共线性及其处理

当回归模型中使用多个自变量时，如果模型中两个或两个以上的自变量彼此相关，则称此回归模型中存在**多重共线性**

#### 多重共线性及其识别

##### 多重共线性所产生的问题

多重共线性会带来以下问题：

-   变量之间高度相关时，可能会造成回归结果的混乱，甚至会把分析带入歧途。

-   多重共线性可能对参数估计值的正负号产生较大的影响。

##### 多重共线性的识别和处理

识别多重线性的方法如下：

-   对回归模型中各对自变量之间的相关系数进行检验

-   考察各回归系数的显著性

-   分析回归系数的正负号

-   用容忍度和方差膨胀因子来识别共线性

#### 变量选择与逐步回归

在多元线性回归模型中，变量的选择方法有**向前选择**，**向后剔除**，**逐步回归**等。

##### 向前选择

向前选择是从模型中没有自变量开始，按下列步骤选择自变量来拟合模型: 首先， 分别拟合因变量$y$对$k$个自变量$(x_1,x_2,...,x_k)$的一元线性回归模型，共有$k$个，然后找出F统计量的值最大的(或P值最小的)模型及其自变量2i并将该自变量首先引人模型(如果所有模型均无统计上的显著性，则运算过程终止，没有模型被拟合).

其次，在模型已经引人$x_i$的基础上，再分别拟合引人模型外的k-1个自变量 $(x_1,\ldots ,x_{i-1},x_{i+1},\ldots ,x_k)$的回归模型，即自变量组合为$x_i+ x_1,x_i+x_{i-1},x_i + x_{i+1},... ,x_i+x_k$的$k-1$个回归模型，然后分别考察这k-1个模型，挑选出F统计量的值最大的(或P值最小的)含有两个自变量的模型，并将使F值最大的(或P值最小的)那个自变量$x_j$引人模型.如果除$x_i$之外的$k-1$个自变量中没有一个是统计上显著的，则运算过程终止.如此反复进行，直至模型外的自变量均无统计显著性为止.

向前选择法的特点是:只要某个自变量被增加到模型中，这个变量就-定会保留在模型中.

##### 向后剔除

与向前选择法相反，向后剔除的基本过程如下:

首先，拟合因变量对所有k个自变量的回归模型.然后考察$p(p<k)$个去掉一个 自变量的模型(这些模型中的每一个都有$k-1$个自变量)，使模型的SSE值减小最小的自变量(F统计量的值最小或其P值最大)被挑选出来并从模型中剔除.

其次，考察$p-1$个去掉一一个自变量的模型(这些模型中的每一个都有$k-2$个自变量)，使模型的SSE值减小最少的自变量被挑选出来并从模型中剔除.如此反复进行，直至剔除一个自变量不会使SSE显著减小为止.这时，模型中所剩的自变量都是显著的.

向后剔除法的特点是:只要某个自变量被从模型中剔除，这个变量就不会再进人模型中.

##### 逐步回归

逐步回归是避免多重共线性的另一种有效方法，它将上述两种方法结合起来筛 选自变量前两步与向前选择法相同，不过在新增加一个自变量后， 它会对模型中所者的变量重新进行考察，看看有没有可能别除某个自变量如果在新增加一个自变批房前面增加m的某个自变量对模型的贡献变得不显著，这个变量就会被剔除，按此方法不停地增加变量并考虑剔除之前增加的变量的可能性，直至增加变量已经不能导致SSE显著减少(这个过程可通过F检验来完成)

逐步回归法的特点是:在前面步骤中增加的自变量在后面的步骤中有可能被剔险而在前面步骤中剔除的自变量在后面的步骤中也可能重新进人到模型中

利用逐步回归选择自变量的标准不同，得到的最终模型也就不同R中的逐步回 归以赤池信息准则(Akaike's information criterion,AIC)为选择标准，选择使AIC最小的变量建立模型、赤池信息准则也称为AIC准则，它由日本学者赤池于1973年提出，除应用于线性模型的变量筛选外，还应用于时间序列自回归模型阶数的确定、 AIC由两部分组成:一部分反映模型的拟合精度; 另一部分反映模型中参数的个数，即模型的繁简程度AIC的值越小，表示报合的模型精度越高而且越简洁当用最小二乘法报合模型时，计算公式为:

$$
AIC = n\ln(\frac{SSE}{n}) + 2p
$$ 式中,n为样本量: p为模型中参数的个数(包括常数项).

### 相对重要性和模型比较

#### 自变量的相对重要性

哪些自变量对因变量的预测相对来说更重要，哪些相对来说不重要，了解这一问题对建模会有一定的参考价值，如果各自变量之间独立，那么根据自变量与因变量之间的相关系数大小就可以对重要性做出排序，相关系数大的显然更重要。但实际问题中，各自变量之间往往有一定的相关性，这就会使评估变得复杂很多.评估自变量相对重要性的方法之一就是比较**标准化回归系数**(standardized regression cofitient).

标准化回归系数是将因变量和所有自变量都标准化后进行回归得到的回归系数.计算标准化回归系数时，[首先将因变量和各个自变量进行标准化\@处理](mailto:首先将因变量和各个自变量进行标准化@处理){.email}，然后根据标 准化后的值进行回归，得到的方程称为**标准化回归方程**(standardized regression equartion),该方程中的回归系数就是标准化回归系数，用$\overline{\beta}$表示，$\overline{\beta}_i$的含义是:在其他自变量取值不变的条件下，自变量$x_i$每变动一个标准差，因变量平均变动$\overline{\beta}_i$个标准差。显然，$\overline{\beta}_i$的绝对值越大，则说明该自变量$x_i$对自变量的影响就越大，相对于其他自变量而言，他对因变量的预测也就越重要。

#### 模型比较

在多元线性回归建模中，如果一个模型包含另一个模型的所有项，并且至少有一个额外项，称这两个模型是**嵌套模型**。在嵌套模型中，包含所有项的模型称为**完全模型**，相对完全模型较简单的模型称为**简化模型**。

进行上述检验的步骤是:首先，用最小二乘法拟合简化模型，并计算相应的残差平方和($SSE_R$);其次，拟合完全模型，并计算出它的残差平方和($SSE_F$);然后计算出一者的差值($SSE_R一SSE_F$)进行比较，如果二次项对模型有贡献，那么$SSE_R$应该比$SSE_F$小很多，二者相差越大，说明完全模型比简化模型提供了越多的信息。检验统计量： $$
F = \frac{(SSE_R - SSE_F)/(k - g)}{SSE_F/(n - k - 1)} ~ F(k - g, n - k - 1)
$$

完全模型中的参数个数(包含常数项)为$(k+1)$,简化模型中的参数个数为$(g+1)$.如果检验的P值很小，就拒绝$H_0$,表示完全模型比简化模型的拟合效果要好;如果P值较大，不拒绝$H_0$,表示简化模型和完全模型的拟合效果一样好.由于建模时将更多的自变量引入模型，不仅增加了建模的复杂性，而且可能造成解释上的困难因此，当不能拒绝$H_0$时，就考虑采用简化模型.


### Logistic回归

**Logistic回归**之所以称为"回归"，是因为人们曾试图用线性回归的方式处理因变量仅有两个不同值的情况(假定因变量为y,自变量为$x_1,\ldots,x_2,...,x_p$).在线性回归中，如果因变量η只有0和1两个值，即使用线性回归训练出模型，其预测值不会在0和1之间，更不会仅限于0和1两个值.此外，诸如正态分布等假定对于两个值的变量更不合适.因此 人们把因变量取两个值考虑成Bernoulli 试验Bernoulli(p), 等价于n= 1时的二项分布 Bernoulli(p),这里Beenoulli试验的"成功"的概率为p(比如p = P(y = 1).如果把P的**优势**(ood) p/(1 - p)的对数ln(p/(1 - p))(称为优势比(dds ratio))作为自变量的线性函数，这就是Logistic回归模型： $$
ln(\frac{p}{1-p}) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots +\beta_px_p = X\beta
$$ 或者 $$
p = \frac{e^{X\beta}}{1+e^{X\beta}}
$$ 实际上，Logistic回归是广义线性模型的一个特例。广义线性模型适用于因变量作为指数分布族的情况，其一般的模型**因变量分布的期望E(y)的某个函数g(E(y))为自变量的线性组合**： $$
g(E(y)) = X\beta
$$ 这里的函数g称为**连接函数**. Logistic回归中的bernoulli分布的期望为p, 连接函数为对数优势$g(p) = ln(p/(1-p))$.

#### 分组数据的Logistics回归模型

针对0-1型因变量产生的问题，我们对回归模型应该做出两个方面的改进。

第一是回归函数应该改用限制在$[0,1]$区间内的连续曲线，而不能再沿用直线回归方程。我们常用的函数为Sigmoid函数： $$
f(x) =\frac{e^x}{1+e^x}  = \frac{1}{1+e^{-x}}
$$

第二，因变量$y_i$本身只取0，1两个离散值，不适于直接作为回归模型中的因变量，由于回归函数$E(y_i) = \pi_i = \beta_0+\beta_1x_i$表示在自变量为$x_i$的条件下$y_i$的平均值，而$y_i$是0-1型随机变量，因而$E(y_i) = \pi_i$就是在自变量为$x_i$的条件下$y_i$等于1的比例.

#### 未分组数据的Logistics回归模型

设$y$是0～1型变量，$x_1,x_2,\cdots,x_p$是与$y$相关的确定性变量，n组观测数据为$(x_{i1},x_{i2},\cdots,x_{ip};y_i)(i=1,2,\cdots,n)$,其中$y_1,y_2,\dots,y_n$是取值为0或1的随机变量，$y_i$与$x_{i1},x_{i2},\cdots,x_{ip}$的关系如下 $$
E(y_i) = \pi_i = f(\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip})
$$ 其中，函数$f(x)$是值域在$[0,1]$区间内的单调增函数。对于logistic回归 $$
f(x) = \frac{e^x}{1+e^x}
$$ 于是$y_i$是均值为$\pi_i = f(\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip})$的0-1型分布，概率函数为 $$
P(y_i=1)=\pi_i\\
P(y_i=0)=1-\pi_i
$$ 可以把$y_i$的概率函数合写为 $$
P(y_i) = \pi_i^{y_i}(1-\pi_i)^{1-y_i},y_i=0,1,i=1,2,\cdots,n
$$ 于是，$y_1,y_2,\cdots,y_n$的似然函数为 $$
L = \prod\limits_{i=1}^{n}P(y_i) = \prod\limits_{i=1}^{n}\pi_i^{y_i}(1-\pi_i)^{1-y_i}
$$ 对似然函数取自然对数，得 $$
lnL = \sum_{i=1}^{n}[y_iln\pi_i+(1-y_i)ln(1-\pi_i)]\\
   =  \sum_{i=1}^{n}[y_iln\frac{\pi_i}{1-\pi_i}+ln(1-\pi_i)]
$$ 对于logistic回归，将 $$
\pi_i = \frac{exp(\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip})}{1+exp(\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip})}
$$ 代入得

$$
lnL = \sum_{i=1}^{n}[y_i(\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip}) - ln(1+ exp(\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip}))]
$$ 极大似然估计就是选取$\beta_0,\beta_1,\beta_2,\cdots,\beta_p$的估计值$\hat{\beta}_0,\hat{\beta}_1,\cdots,\hat{\beta}_p$使得上式达到最大。
