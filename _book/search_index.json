[["关系分析.html", "第 6 章 关系分析 6.1 相关分析 6.2 一元与多元回归 6.3 普通最小二乘法回归 6.4 Logistic回归", " 第 6 章 关系分析 6.1 相关分析 6.1.1 概念 直线相关：是一种研究两个变量之前的线性相关关系（相关的方向和密切程度）的一种统计方法 注：相关系数等于0时，只能说明两变量间无直线关系，不能说两变量无关 6.1.2 相关系数及其计算 相关系数：又称Pearson积差相关系数，是说明具有直线相关关系的两个数值变量之间相关的方向和密切程度的统计量。 2.计算公式： \\[ r = \\frac{l_{XY}}{\\sqrt{l_{XX}l_{YY}}} = \\frac{\\sum (X-\\bar{X})(Y-\\bar{Y})}{\\sqrt{\\sum (X-\\bar{X})^2\\sum (Y-\\bar{Y})^2}} \\] 其中\\(l_{XY}\\)表示\\(X\\)与\\(Y\\)的离均差积和，\\(l_{XX}\\)表示\\(X\\)的离均差平方和，\\(l_{YY}\\)表示\\(Y\\)的离均差平方和。 相关系数是没有单位的，取值范围为\\([-1,1]\\). 当\\(r &gt; 0\\)时，两随机变量为正相关；当\\(r &lt;0\\)时，两随机变量为负相关; 当\\(|r| = 1\\)时，两随机变量完全相关；当\\(|r| = 0\\)时, 两随机变量无直线关系. 6.2 一元与多元回归 ###概念 1.回归：是分析研究变量与变量之间的关系的一种行为，也可以说是回归于事物本来的面目！ 2.回归分析：指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。 回归分析按照涉及的变量的多少，分为一元回归和多元回归分析；按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。 6.3 普通最小二乘法回归 普通最小二乘法回归（ordinary least squares，OLS）是一类服务于正态响应变量的模型框架。包括：简单线性回归、多项式回归和多元线性回归；OLS回归是目前最常见的统计分析方法。 ###简单线性回归模型 简单回归模型 \\[ y = \\beta_0 +\\beta_1x+u \\] 零条件均值假设下： \\[ E(u) = 0 \\\\ Cov(x,u) = E(xu) = 0 \\] 矩估计: \\[ \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{\\beta}_0 -\\hat{\\beta}_1x_i) = 0 \\\\ \\frac{1}{n}\\sum_{i=1}^{n}x_i(y_i-\\hat{\\beta}_0 -\\hat{\\beta}_1x_i) = 0 \\] 得到估计的斜率： \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}{n}(x_i-\\bar{x})^2} \\] 定义总平方和(total sum of squares, SST)，解释平方和(explained sum of squares.SSE), 残差平方和(residual sum of squares, SSR): \\[ SST = \\sum_{i=1}{n}(y_i-\\bar{y})^2\\\\ SSE = \\sum_{i=1}{n}(\\hat{y}_i-\\bar{y})^2\\\\ SSR = \\sum_{i=1}{n}\\hat{u}_i^2\\\\ SST = SSE + SSR \\] 拟合优度： \\[ R^2 = SSE/SST = 1 - SSR/SST \\] 6.3.1 多元回归分析 估计： \\[ y = \\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_kx_k + u \\] \\(\\hat{\\beta}_i\\)意为排除了其他变量之后，\\(x_i\\)对\\(y\\)的影响。 利用现有样本，以\\(x_i\\)为因变量，其他变量为自变量做回归，得到残差；再将\\(y\\)对OLS残差进行回归就能得到\\(\\hat{\\beta}_i\\)。 6.4 Logistic回归 6.4.1 分组数据的Logistics回归模型 针对0-1型因变量产生的问题，我们对回归模型应该做出两个方面的改进。 第一是回归函数应该改用限制在\\([0,1]\\)区间内的连续曲线，而不能再沿用直线回归方程。我们常用的函数为Sigmoid函数： \\[ f(x) =\\frac{e^x}{1+e^x} = \\frac{1}{1+e^{-x}} \\] 第二，因变量\\(y_i\\)本身只取0，1两个离散值，不适于直接作为回归模型中的因变量，由于回归函数\\(E(y_i) = \\pi_i = \\beta_0+\\beta_1x_i\\)表示在自变量为\\(x_i\\)的条件下\\(y_i\\)的平均值，而\\(y_i\\)是0-1型随机变量，因而\\(E(y_i) = \\pi_i\\)就是在自变量为\\(x_i\\)的条件下\\(y_i\\)等于1的比例. 6.4.2 未分组数据的Logistics回归模型 设\\(y\\)是0～1型变量，\\(x_1,x_2,\\cdots,x_p\\)是与\\(y\\)相关的确定性变量，n组观测数据为\\((x_{i1},x_{i2},\\cdots,x_{ip};y_i)(i=1,2,\\cdots,n)\\),其中\\(y_1,y_2,\\dots,y_n\\)是取值为0或1的随机变量，\\(y_i\\)与\\(x_{i1},x_{i2},\\cdots,x_{ip}\\)的关系如下 \\[ E(y_i) = \\pi_i = f(\\beta_0+\\beta_1x_{i1}+\\cdots+\\beta_px_{ip}) \\] 其中，函数\\(f(x)\\)是值域在\\([0,1]\\)区间内的单调增函数。对于logistic回归 \\[ f(x) = \\frac{e^x}{1+e^x} \\] 于是\\(y_i\\)是均值为\\(\\pi_i = f(\\beta_0+\\beta_1x_{i1}+\\cdots+\\beta_px_{ip})\\)的0-1型分布，概率函数为 \\[ P(y_i=1)=\\pi_i\\\\ P(y_i=0)=1-\\pi_i \\] 可以把\\(y_i\\)的概率函数合写为 \\[ P(y_i) = \\pi_i^{y_i}(1-\\pi_i)^{1-y_i},y_i=0,1,i=1,2,\\cdots,n \\] 于是，\\(y_1,y_2,\\cdots,y_n\\)的似然函数为 \\[ L = \\prod\\limits_{i=1}^{n}P(y_i) = \\prod\\limits_{i=1}^{n}\\pi_i^{y_i}(1-\\pi_i)^{1-y_i} \\] 对似然函数取自然对数，得 \\[ lnL = \\sum_{i=1}^{n}[y_iln\\pi_i+(1-y_i)ln(1-\\pi_i)]\\\\ = \\sum_{i=1}^{n}[y_iln\\frac{\\pi_i}{1-\\pi_i}+ln(1-\\pi_i)] \\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
